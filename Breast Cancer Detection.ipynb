{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Challenge:\n",
    "\n",
    "Your task is to develop a model that predicts whether a biopsied breast cell is benign (not harmful) or malignant (cancerous), given a set of attributes about the cell.\n",
    "\n",
    "There are many ways you can explore, visualize, engineer your features, and tell a story with this data! Being able to clearly communicate your thought process is one of the most important parts of a data challenge. Some important questions to think about are: how can you best explore the data? Why did you select your particular model? How did you validate your model?\n",
    "\n",
    "Please code and annotate your analysis in an Jupyter notebook.\n",
    "\n",
    "The dataset consists of 699 cells for which you have the following features:\n",
    "\n",
    "Sample code number: id number\n",
    "Clump Thickness: 1 - 10\n",
    "Uniformity of Cell Size: 1 - 10\n",
    "Uniformity of Cell Shape: 1 - 10\n",
    "Marginal Adhesion: 1 - 10\n",
    "Single Epithelial Cell Size: 1 - 10\n",
    "Bare Nuclei: 1 - 10\n",
    "Bland Chromatin: 1 - 10\n",
    "Normal Nucleoli: 1 - 10\n",
    "Mitoses: 1 - 10\n",
    "Class: (2 for benign, 4 for malignant)\n",
    "The dataset is available here: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "#import statsmodels.discrete.discrete_model as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.naive_bayes import GaussianNB as GNB\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# useful for evaluating predictive capabilities \n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# not using this but good to keep in mind!\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# will use this for preprocessing the data\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'clump_thickness', 'uniform_size', 'uniform_shape', 'adhesion', 'epithel_size', 'bland_chromatin', 'nucleoli', 'mitoses', 'Class']\n"
     ]
    }
   ],
   "source": [
    "# from data description, these are the column names\n",
    "names_list = ['id','clump_thickness','uniform_size','uniform_shape','adhesion','epithel_size',\\\n",
    "              'bare_nuclei','bland_chromatin','nucleoli','mitoses','Class']\n",
    "\n",
    "# import the data into pandas dataframe\n",
    "link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data'\n",
    "f = requests.get(link).content\n",
    "data = pd.read_csv(io.StringIO(f.decode('utf-8')), names = names_list)\n",
    "\n",
    "# keep only numeric types\n",
    "data = data.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets look at the histograms\n",
    "data.hist()\n",
    "plt.figure(1)\n",
    "plt.savefig('histograms.pdf')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable 'Class' is a binary classifier, so my first thought is to use a logistic regression.\n",
    "\n",
    "I'll start initially by including all of the independent variables, except for the 'id' variable as this should be irrelevant to the prediction of the tumor size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.139046\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  Class   No. Observations:                  559\n",
      "Model:                          Logit   Df Residuals:                      552\n",
      "Method:                           MLE   Df Model:                            6\n",
      "Date:                Wed, 21 Feb 2018   Pseudo R-squ.:                  0.7831\n",
      "Time:                        20:11:35   Log-Likelihood:                -77.727\n",
      "converged:                       True   LL-Null:                       -358.30\n",
      "                                        LLR p-value:                5.588e-118\n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "const              -6.7598      0.642    -10.537      0.000      -8.017      -5.502\n",
      "clump_thickness     0.5875      0.088      6.669      0.000       0.415       0.760\n",
      "uniform_size        0.3509      0.128      2.743      0.006       0.100       0.602\n",
      "uniform_shape       0.3593      0.119      3.021      0.003       0.126       0.592\n",
      "epithel_size        0.2003      0.096      2.093      0.036       0.013       0.388\n",
      "adhesion            0.2417      0.087      2.792      0.005       0.072       0.411\n",
      "nucleoli            0.2155      0.096      2.241      0.025       0.027       0.404\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "# lets first start with a logistic regression to see if we can \n",
    "# accurately classify the data into benign or malignant\n",
    "# first change the benign and malignant scores to 0 and 1, respectively \n",
    "data['Class'] = data['Class'].replace(to_replace=[2,4], value = [0,1])\n",
    "\n",
    "#### need to split up training/testing set 80/20 ########\n",
    "train = data.sample(frac = 0.8, random_state = 1)\n",
    "test = data.loc[~data.index.isin(train.index)]\n",
    "\n",
    "### train a logistic regression on a selection of the columns ###\n",
    "train_cols = ['clump_thickness', 'uniform_size', 'uniform_shape', 'epithel_size','adhesion','nucleoli']\n",
    "\n",
    "# subset the set used for the training data and add a constant\n",
    "logit_train_data = train[train_cols]\n",
    "logit_train_data = sm.add_constant(logit_train_data)\n",
    "\n",
    "logit = sm.Logit(train['Class'],logit_train_data)\n",
    "result = logit.fit()\n",
    "\n",
    "print result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the test data to predict the target variable on the test set\n",
    "logit_test_data = test[train_cols]\n",
    "logit_test_data = sm.add_constant(logit_test_data)\n",
    "\n",
    "# get the prediction and round to the nearest integer \n",
    "# since this is a binary classification\n",
    "predict = result.predict(logit_test_data)\n",
    "\n",
    "round_predict = [round(x) for x in predict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the tested independent variables have p-values less than 0.05 which indicates that they are significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's look at the predictions by histogramming them\n",
    "plt.figure(2)\n",
    "\n",
    "bins = np.arange(0,1.1,.1)\n",
    "plt.hist(round_predict,bins,label = 'LR predicted', alpha = 0.3)\n",
    "plt.hist(test['Class'],bins,label ='real', alpha = 0.3)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('prediction')\n",
    "plt.title(\"Histogram of LR prediction results\")\n",
    "plt.legend()\n",
    "plt.savefig('LogReg_Results.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LR confusion matrix:\n",
      "[[86  3]\n",
      " [ 7 44]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.97      0.95        89\n",
      "          1       0.94      0.86      0.90        51\n",
      "\n",
      "avg / total       0.93      0.93      0.93       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"\\n LR confusion matrix:\\n\",(confusion_matrix(y_test,round_predict))\n",
    "print(classification_report(y_test,round_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this model OVERESTIMATES the number of benign tumors, and underestimates the number of malignant tumors. That's a pretty big deal... For this kind of problem, the patient and doctor should probably err on the side of caution and choose a prediction method that would diagnose a benign tumor as malignant rather than vice versa. \n",
    "\n",
    "Another approach would be to use Linear and Quadratic Discriminant Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train linear discriminant analysis\n",
    "lda = LDA()\n",
    "lda_model = lda.fit(train[train_cols],train['Class'])\n",
    "results_lda = lda_model.predict(test[train_cols])\n",
    "\n",
    "# train quadratic discriminant analysis\n",
    "qda = QDA()\n",
    "qda_model = qda.fit(train[train_cols],train['Class'])\n",
    "results_qda = qda_model.predict(test[train_cols])\n",
    "\n",
    "plt.figure(3)\n",
    "#plt.hist(round_predict,bins,label = 'LR predicted', alpha = 0.3)\n",
    "plt.hist(results_lda,bins,label='LDA predict',alpha =0.3)\n",
    "plt.hist(test['Class'],bins,label ='real', alpha = 0.3)\n",
    "plt.legend()\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('prediction')\n",
    "plt.title(\"Histogram of LDA prediction results\")\n",
    "plt.savefig('LDA_Results.pdf')\n",
    "\n",
    "plt.figure(4)\n",
    "plt.hist(results_qda,bins,label='QDA predict',alpha =0.3)\n",
    "plt.hist(test['Class'],bins,label ='real', alpha = 0.3)\n",
    "plt.legend()\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('prediction')\n",
    "plt.title(\"Histogram of QDA prediction results\")\n",
    "plt.savefig('QDA_Results.pdf')\n",
    "\n",
    "plt.figure(5)\n",
    "plt.hist(round_predict,bins,label = 'LR predicted', alpha = 0.4)\n",
    "plt.hist(results_lda,bins,label='LDA predict',alpha =0.3)\n",
    "plt.hist(results_qda,bins,label='QDA predict',alpha =0.2)\n",
    "plt.hist(test['Class'],bins,label ='real', alpha = 0.1)\n",
    "plt.title(\"Histogram of multiple prediction results\")\n",
    "plt.legend()\n",
    "plt.savefig('Combined_Results.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LDA confusion matrix:\n",
      "[[87  2]\n",
      " [ 8 43]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95        89\n",
      "          1       0.96      0.84      0.90        51\n",
      "\n",
      "avg / total       0.93      0.93      0.93       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"\\n LDA confusion matrix:\\n\",(confusion_matrix(y_test,results_lda))\n",
    "print(classification_report(y_test,results_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " QDA confusion matrix:\n",
      "[[85  4]\n",
      " [ 1 50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.96      0.97        89\n",
      "          1       0.93      0.98      0.95        51\n",
      "\n",
      "avg / total       0.97      0.96      0.96       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"\\n QDA confusion matrix:\\n\",(confusion_matrix(y_test,results_qda))\n",
    "print(classification_report(y_test,results_qda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QDA gets much better. In fact, it starts to overpredict the malignant tumors vs. benign tumors and has a 97% accuracy rate. \n",
    "\n",
    "Let's try a few more such as Gaussian Naive Bayes which assumes that the liklihood of the features is assumed to be Gaussian. I'm not really sure we can say that. Perhaps we should check the features for normality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the Shapiro test for the variables:\n",
      "\n",
      "clump_thickness (0.877661943435669, 5.2220136990493934e-23)\n",
      "uniform_size (0.6862915754318237, 4.978149239932788e-34)\n",
      "uniform_shape (0.7180565595626831, 1.1502892540151622e-32)\n",
      "epithel_size (0.6991004943847656, 1.7103106686548345e-33)\n",
      "adhesion (0.6512163877487183, 2.0616883781547084e-35)\n",
      "nucleoli (0.6399070024490356, 7.802240438649756e-36)\n",
      "\n",
      "Now for the Gaussian Naive Bayes...\n",
      "Number of mislabeled points out of a total 699 points : 34\n"
     ]
    }
   ],
   "source": [
    "# check for normality of the values in the training column\n",
    "print \"Check the Shapiro test for the variables:\\n\"\n",
    "for i in train_cols:\n",
    "    print i, stats.shapiro(data[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these variables are not very close to normally distributed as indicated by the W value of the Shapiro test. (W should be near 1). Therefore, the Gaussian Naive Bayes method is making some pretty major assumptions about the input variables. It seems to do OK at predicting the target variables. \n",
    "\n",
    "I tried the multinomial Naive Bayes below, which is suited for discrete variables, but this definitely did not improve predictions results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now for the Gaussian Naive Bayes...\n",
      "Number of mislabeled points out of a total 699 points : 34\n"
     ]
    }
   ],
   "source": [
    "gnb = GNB()\n",
    "gnb_pred = gnb.fit(data[train_cols], data['Class']).predict(data[train_cols])\n",
    "\n",
    "print \"\\nNow for the Gaussian Naive Bayes...\"\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (data[train_cols].shape[0],(data['Class'] != gnb_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now for the Multinomial Naive Bayes...\n",
      "Number of mislabeled points out of a total 699 points : 125\n"
     ]
    }
   ],
   "source": [
    "mnb = MNB()\n",
    "mnb_pred = mnb.fit(data[train_cols], data['Class']).predict(data[train_cols])\n",
    "\n",
    "print \"\\nNow for the Multinomial Naive Bayes...\"\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (data[train_cols].shape[0],(data['Class'] != mnb_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use this to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = train[train_cols]\n",
    "X_test = test[train_cols]\n",
    "\n",
    "y_train = train['Class']\n",
    "y_test = test['Class']\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train the classifier\n",
    "# the hidden layers number is tricky - not sure how to pick these\n",
    "# Gave error when I used (30,30,30)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10,10))\n",
    "mlp_model = mlp.fit(X_train,y_train)\n",
    "\n",
    "# predict on the test set\n",
    "mlp_predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MLP confusion matrix:\n",
      "[[85  4]\n",
      " [ 4 47]]\n"
     ]
    }
   ],
   "source": [
    "# this is the confusing matrix\n",
    "#print \"f1 score: \", f1_score(y_test, mlp_predictions,average = 'micro')\n",
    "print \"\\n MLP confusion matrix:\\n\",(confusion_matrix(y_test,mlp_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.97      0.97        89\n",
      "          1       0.94      0.96      0.95        51\n",
      "\n",
      "avg / total       0.96      0.96      0.96       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is the classification report\n",
    "print(classification_report(y_test,mlp_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP gives us a 96% accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's try K-Nearest Neighbors for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
